{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "fig_base = Path(\".\").absolute().parent / \"reports\" / \"figures\"\n",
    "\n",
    "fig_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the `round_table` augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitatively analyze trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.visualization.render_mp4 import (\n",
    "    mp4_from_pickle_jupyter,\n",
    "    unpickle_data,\n",
    "    pickle_data,\n",
    ")\n",
    "from src.common.files import get_raw_paths\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "base_dir = Path(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_raw_paths(\n",
    "    environment=\"sim\",\n",
    "    demo_source=\"augmentation\",\n",
    "    demo_outcome=\"success\",\n",
    "    task=\"round_table\",\n",
    "    randomness=\"low\",\n",
    ")\n",
    "\n",
    "random.shuffle(paths)\n",
    "\n",
    "len(paths), paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teleop_paths = get_raw_paths(\n",
    "    environment=\"sim\",\n",
    "    demo_source=\"teleop\",\n",
    "    demo_outcome=\"success\",\n",
    "    task=\"round_table\",\n",
    "    randomness=\"low\",\n",
    ")\n",
    "\n",
    "random.shuffle(teleop_paths)\n",
    "\n",
    "len(teleop_paths), teleop_paths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in tqdm(teleop_paths):\n",
    "    data = unpickle_data(path)\n",
    "    idxs = np.where(np.array(data[\"augment_states\"]) == 1)[0]\n",
    "    if len(idxs) != 3:\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = defaultdict(list)\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    data = unpickle_data(path)\n",
    "    images[data.get(\"critical_state\")].append(data[\"observations\"][0][\"color_image2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, len(v)) for k, v in images.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Iterate over the keys and images in the images dictionary\n",
    "for i, (key, image_list) in enumerate(images.items()):\n",
    "    if key is None:\n",
    "        continue\n",
    "\n",
    "    # Create a 3-by-3 grid of subplots with no space between axes\n",
    "    img_list = image_list[:9]\n",
    "\n",
    "    h, w = img_list[0].shape[:2]\n",
    "\n",
    "    # Center crop each image to be square\n",
    "    img_list = [img[:, (w - h) // 2 : (w + h) // 2] for img in img_list]\n",
    "\n",
    "    # Remove 10 pixels from each edge of each image\n",
    "    img_list = [img[10:-10, 10:-10] for img in img_list]\n",
    "\n",
    "    # Concatenate the images into a single 3-by-3 image\n",
    "    img = [np.concatenate(img_list[i : i + 3], axis=1) for i in range(0, 9, 3)]\n",
    "    img = np.concatenate(img, axis=0)\n",
    "\n",
    "    # Create a new figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Remove the x and y ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Remove everything\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Save without a white edge around the image\n",
    "    plt.savefig(\n",
    "        fig_base / f\"augmentation_grid_{i}.png\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot coverage of new trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from src.common.files import get_processed_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_path, teleop_path = sorted(\n",
    "    get_processed_paths(\n",
    "        environment=\"sim\",\n",
    "        demo_source=[\"teleop\", \"augmentation\"],\n",
    "        demo_outcome=\"success\",\n",
    "        task=\"round_table\",\n",
    "        randomness=\"low\",\n",
    "    )\n",
    ")\n",
    "\n",
    "aug_path, teleop_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_aug = zarr.open(str(aug_path), mode=\"r\")\n",
    "z_teleop = zarr.open(str(teleop_path), mode=\"r\")\n",
    "\n",
    "ends_aug = z_aug[\"episode_ends\"][:]\n",
    "ends_teleop = z_teleop[\"episode_ends\"][:]\n",
    "\n",
    "pos_teleop = z_teleop[\"robot_state\"][:, :3]\n",
    "pos_aug = z_aug[\"robot_state\"][:, :3]\n",
    "\n",
    "\n",
    "# Split the data into episodes\n",
    "pos_teleop = np.split(pos_teleop, ends_teleop[:-1])\n",
    "pos_aug = np.split(pos_aug, ends_aug[:-1])\n",
    "\n",
    "# Filter out only episodes with critical state index (i.e., not -1)\n",
    "cs_idx = z_aug[\"critical_state_id\"][:]\n",
    "aug_for_state = defaultdict(list)\n",
    "for i, (pos, idx) in enumerate(zip(pos_aug, cs_idx)):\n",
    "    if idx != -1:\n",
    "        aug_for_state[idx].append(pos)\n",
    "\n",
    "cs_insert_leg = aug_for_state[0]\n",
    "cs_grasp_base = aug_for_state[1]\n",
    "cs_insert_base = aug_for_state[2]\n",
    "\n",
    "# Count number in each state\n",
    "len(cs_insert_leg), len(cs_grasp_base), len(cs_insert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_teleop.critical_state_idxs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat them together again\n",
    "# pos_teleop = np.concatenate(pos_teleop)\n",
    "# pos_aug = np.concatenate(pos_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ends_teleop), len(pos_teleop), len(ends_aug), len(pos_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the state-space coverage in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add teleop scatter plot\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=pos_teleop.T[0],\n",
    "        y=pos_teleop.T[1],\n",
    "        z=pos_teleop.T[2],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            opacity=0.3,\n",
    "            # color=\"#BCD3FF\",\n",
    "        ),\n",
    "        name=f\"Teleop (n={len(ends_teleop)})\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add augmentation scatter plot\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(\n",
    "        x=pos_aug.T[0],\n",
    "        y=pos_aug.T[1],\n",
    "        z=pos_aug.T[2],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=2,\n",
    "            opacity=0.5,\n",
    "            # color=\"#FFB8B8\",\n",
    "        ),\n",
    "        name=f\"Augmentation (n={len(ends_aug)})\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update the layout to make it look nice and square\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"x\",\n",
    "        yaxis_title=\"y\",\n",
    "        zaxis_title=\"z\",\n",
    "        aspectmode=\"cube\",  # Ensures equal aspect ratio for a square look\n",
    "        camera=dict(\n",
    "            up=dict(x=0, y=0, z=0.7),  # Sets the z-axis to be up\n",
    "            center=dict(x=0, y=0, z=0),  # Centers the view\n",
    "            eye=dict(x=0.9, y=0.9, z=0.9),  # Adjust these values to zoom in\n",
    "        ),\n",
    "    ),\n",
    "    legend=dict(yanchor=\"top\", y=0.9, xanchor=\"left\", x=0.01, font=dict(size=24)),\n",
    "    margin=dict(l=0, r=0, b=0, t=0),  # Reduce default margins\n",
    "    width=800,  # Adjust figure width\n",
    "    height=800,  # Adjust figure height to make it square\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(zaxis=dict(range=[0, 0.4]))  # Focuses the z-axis to show only 0 to 50\n",
    ")\n",
    "\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "# To save the figure\n",
    "fig.write_image(str(fig_base / \"teleop_augmentation.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a big grid of augmented trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs2 = z_aug[\"color_image2\"][:]\n",
    "\n",
    "imgs2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split it into episodes\n",
    "imgs2 = np.split(imgs2, ends_aug[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imgs2), imgs2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vids = 15\n",
    "\n",
    "vids = []\n",
    "for i in range(0, len(imgs2), n_vids):\n",
    "    imgs = imgs2[i : i + n_vids]\n",
    "\n",
    "    vid = np.concatenate(imgs, axis=0)\n",
    "\n",
    "    vids.append(vid)\n",
    "\n",
    "len(vids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_height, crop_width = 224, 224\n",
    "nrows, ncols = 5, 9\n",
    "h, w = vids[0].shape[1:3]\n",
    "\n",
    "h, w, crop_height, crop_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h - crop_height) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids = vids[: nrows * ncols]\n",
    "\n",
    "# Cut the videos to the same length\n",
    "min_len = min(len(vid) for vid in vids)\n",
    "\n",
    "# Cut and center crop the videos\n",
    "vids = [\n",
    "    vid[\n",
    "        :min_len,\n",
    "        (h - crop_height) // 2 : (h + crop_height) // 2,\n",
    "        (w - crop_width) // 2 : (w + crop_width) // 2,\n",
    "    ]\n",
    "    for vid in vids\n",
    "]\n",
    "\n",
    "len(vids), vids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the videos into a single 4-by-4 video\n",
    "vid = [\n",
    "    np.concatenate(vids[i : i + ncols], axis=1) for i in range(0, nrows * ncols, ncols)\n",
    "]\n",
    "vid = np.concatenate(vid, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.render_mp4 import create_mp4_jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_mp4_jupyter(vid, \"augmentation_grid.mp4\", fps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make videos of rollouts for all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_teleop = sorted(\n",
    "    get_processed_paths(\n",
    "        environment=\"sim\",\n",
    "        demo_source=[\"teleop\"],\n",
    "        demo_outcome=\"success\",\n",
    "        task=[\"round_table\", \"lamp\", \"square_table\", \"one_leg\"],\n",
    "        randomness=\"low\",\n",
    "    )\n",
    ")\n",
    "\n",
    "z_teleop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render 3 videoes from each task\n",
    "\n",
    "for path in z_teleop:\n",
    "    z = zarr.open(str(path), mode=\"r\")\n",
    "    imgs = z[\"color_image2\"][: z[\"episode_ends\"][2]]\n",
    "\n",
    "    # Split it into episodes\n",
    "    imgs = np.split(imgs, z[\"episode_ends\"][:2])\n",
    "\n",
    "    for i, img in enumerate(imgs[:3]):\n",
    "        create_mp4_jupyter(img, f\"teleop_{path.parts[-5:-1]}_{i}.mp4\", fps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a plot for the Bootstrap experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "x = range(1, 5)  # For example, 4 groups\n",
    "category_a = [5, 10, 15, 20]\n",
    "category_b = [3, 7, 12, 17]\n",
    "category_c = [1, 4, 9, 14]\n",
    "line_data = [10, 15, 20, 25]\n",
    "\n",
    "# Plotting the stacked bars\n",
    "plt.bar(x, category_a, label=\"Category A\", color=\"lightblue\")\n",
    "plt.bar(x, category_b, bottom=category_a, label=\"Category B\", color=\"lightgreen\")\n",
    "# To stack category_c, we need to add the values of category_a and category_b for the bottom parameter\n",
    "combined_bottom = [a + b for a, b in zip(category_a, category_b)]\n",
    "plt.bar(x, category_c, bottom=combined_bottom, label=\"Category C\", color=\"lightcoral\")\n",
    "\n",
    "# Adding the line plot\n",
    "plt.plot(x, line_data, label=\"Line Data\", color=\"black\", marker=\"o\", linestyle=\"--\")\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel(\"X axis\")\n",
    "plt.ylabel(\"Y axis\")\n",
    "plt.title(\"Stacked Bar Chart with Line\")\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "x = range(1, 4)\n",
    "category_a = [10, 10, 10]\n",
    "category_b = [0, 40, 90]\n",
    "line_data = [19, 55, 71]\n",
    "\n",
    "x_labels = [\"Iteration 1\", \"Iteration 2\", \"Iteration 3\"]\n",
    "# x = range(1, 3)\n",
    "# category_a = [10, 10]\n",
    "# category_b = [0, 90]\n",
    "# line_data = [19, 71]\n",
    "\n",
    "# x_labels = [\"Iteration 1\", \"Iteration 2\"]\n",
    "\n",
    "# Creating a subplot\n",
    "fig, ax1 = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "# Plotting the stacked bars on the first axis\n",
    "bar1 = ax1.bar(x, category_a, label=\"Manual demos\", color=\"#BCD3FF\")\n",
    "bar2 = ax1.bar(x, category_b, bottom=category_a, label=\"Synthetic\", color=\"#FFB8B8\")\n",
    "ax1.set_xlabel(\"# Iteration of JUICER\")\n",
    "ax1.set_ylabel(\"Number of Demos\")\n",
    "\n",
    "# Set custom x-axis tick labels\n",
    "ax1.set_xticks(x)  # Set the positions of the x-ticks\n",
    "ax1.set_xticklabels(x_labels)  # Set the custom labels for each tick\n",
    "\n",
    "# Create a second y-axis for the line plot\n",
    "ax2 = ax1.twinx()\n",
    "(line,) = ax2.plot(\n",
    "    x, line_data, label=\"Line Data\", color=\"black\", marker=\"o\", linestyle=\"--\"\n",
    ")\n",
    "ax2.set_ylabel(\"Average Success Rate (%)\")\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "# Format the right y-axis as percent\n",
    "ax2.yaxis.set_major_formatter(PercentFormatter(decimals=0))\n",
    "\n",
    "\n",
    "# Adding labels above each point on the line\n",
    "for i, txt in enumerate(line_data):\n",
    "    ax2.text(x[i], line_data[i] + 2, f\"{txt}%\", ha=\"center\", va=\"bottom\", color=\"black\")\n",
    "\n",
    "\n",
    "# Creating combined legend\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "handles.append(line)  # Adding the line plot handle to the list of handles\n",
    "labels.append(\"Line Data\")  # Adding the line plot label to the list of labels\n",
    "\n",
    "# Displaying the combined legend\n",
    "ax1.legend(handles, labels, loc=\"upper left\")\n",
    "\n",
    "# fig.suptitle(\"Stacked Bar Chart with Line\")\n",
    "\n",
    "plt.savefig(fig_base / \"bootstrap_success.pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(1, 4)\n",
    "category_a = [10, 50, 10]\n",
    "category_b = [0, 0, 90]\n",
    "line_data = [19, 55, 71]\n",
    "\n",
    "x_labels = [\"10 demos\", \"50 demos\", \"10 demos\\n+JUICER\"]\n",
    "\n",
    "# Adjusting category_a and category_b to fit the total heights specified by line_data\n",
    "# Calculate the total of category_a and category_b for each stack\n",
    "totals = [a + b for a, b in zip(category_a, category_b)]\n",
    "\n",
    "# Calculate the adjusted heights\n",
    "adjusted_a = [a / t * l if t else 0 for a, t, l in zip(category_a, totals, line_data)]\n",
    "adjusted_b = [l - a for a, l in zip(adjusted_a, line_data)]\n",
    "\n",
    "# Creating a subplot\n",
    "fig, ax1 = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "# Plotting the adjusted stacked bars on the first axis\n",
    "bar1 = ax1.bar(x, adjusted_a, label=\"Manual demos\", color=\"#BCD3FF\")\n",
    "bar2 = ax1.bar(x, adjusted_b, bottom=adjusted_a, label=\"Synthetic\", color=\"#FFB8B8\")\n",
    "\n",
    "# ax1.set_xlabel(\"# Iteration of JUICER\")\n",
    "ax1.set_ylabel(\"Average Success Rate (%)\")\n",
    "\n",
    "# Set custom x-axis tick labels\n",
    "ax1.set_xticks(x)  # Set the positions of the x-ticks\n",
    "ax1.set_xticklabels(x_labels)  # Set the custom labels for each tick\n",
    "\n",
    "# Adjust y-axis labels to reflect the actual values\n",
    "ax1.set_ylim(\n",
    "    0, max(line_data) + 10\n",
    ")  # Adjust ylim to fit the highest line_data value plus some margin\n",
    "\n",
    "# Add data labels atop each bar\n",
    "for xpos, value in zip(x, line_data):\n",
    "    ax1.text(xpos, value, f\"{value}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Creating legend\n",
    "ax1.legend(loc=\"upper left\", frameon=False)\n",
    "\n",
    "\n",
    "plt.savefig(fig_base / \"bootstrap_success.pdf\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render new rollouts in high resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.visualization.render_mp4 import (\n",
    "    create_mp4_jupyter,\n",
    "    unpickle_data,\n",
    ")\n",
    "import cv2\n",
    "from src.common.files import get_raw_paths\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "base_dir = Path(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rollout_video(demo_outcome=\"success\", task_subset=None, n_videos=1):\n",
    "    for task in tqdm([\"one_leg\", \"round_table\", \"lamp\", \"square_table\"]):\n",
    "\n",
    "        if task_subset is not None and task not in task_subset:\n",
    "            continue\n",
    "\n",
    "        paths = get_raw_paths(\n",
    "            environment=\"sim\",\n",
    "            demo_source=\"rollout\",\n",
    "            demo_outcome=demo_outcome,\n",
    "            task=task,\n",
    "            randomness=\"low\",\n",
    "        )\n",
    "\n",
    "        fps = 20\n",
    "\n",
    "        paths = sorted(paths, reverse=True)[:n_videos]\n",
    "\n",
    "        for i, path in enumerate(paths):\n",
    "            print(f\"Rendering {path}\")\n",
    "            imgs = []\n",
    "            for obs in tqdm(unpickle_data(path)[\"observations\"]):\n",
    "\n",
    "                img = obs[\"color_image2\"]\n",
    "\n",
    "                # Add the \"2x\" text overlay in the lower-right corner\n",
    "                img_with_text = cv2.putText(\n",
    "                    img,\n",
    "                    f\"{fps//10}x\",\n",
    "                    (\n",
    "                        img.shape[1] - 100,\n",
    "                        img.shape[0] - 40,\n",
    "                    ),  # Adjusted position of the text\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,  # Font type\n",
    "                    2,  # Font scale (doubled)\n",
    "                    (255, 255, 255),  # Text color (white)\n",
    "                    4,  # Text thickness (doubled)\n",
    "                    cv2.LINE_AA,  # Line type for better rendering\n",
    "                )\n",
    "\n",
    "                imgs.append(img_with_text)\n",
    "\n",
    "            imgs = np.array(imgs)\n",
    "            create_mp4_jupyter(\n",
    "                imgs, base_dir / f\"rollout_{task}_{demo_outcome}_{i}.mp4\", fps=fps\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render Successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_rollout_video(demo_outcome=\"success\", task_subset=[\"lamp\"], n_videos=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_rollout_video(demo_outcome=\"failure\", task_subset=[\"one_leg\"], n_videos=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render with egocentric view as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_raw_paths(\n",
    "    environment=\"sim\",\n",
    "    demo_source=\"rollout\",\n",
    "    demo_outcome=\"success\",\n",
    "    task=\"round_table\",\n",
    "    randomness=\"low\",\n",
    ")\n",
    "\n",
    "fps = 20\n",
    "n_videos = 1\n",
    "\n",
    "path = sorted(paths, reverse=True)[0]\n",
    "\n",
    "print(f\"Rendering {path}\")\n",
    "imgs = []\n",
    "for obs in tqdm(unpickle_data(path)[\"observations\"]):\n",
    "\n",
    "    img1 = obs[\"color_image1\"]\n",
    "    img2 = obs[\"color_image2\"]\n",
    "\n",
    "    img = np.concatenate([img1, img2], axis=1)\n",
    "\n",
    "    robot_state = obs[\"robot_state\"]\n",
    "    robot_state = [\n",
    "        round(val, 2) for val in robot_state\n",
    "    ]  # Limit to 2 numbers after the dot\n",
    "\n",
    "    # Add black pixels at the top of the image for camera view titles\n",
    "    title_bar_height = 60\n",
    "    title_bar = np.zeros((title_bar_height, img.shape[1], 3), dtype=np.uint8)\n",
    "    img_with_title_bar = np.vstack((title_bar, img))\n",
    "\n",
    "    # Add titles for the camera views\n",
    "    img_with_titles = cv2.putText(\n",
    "        img_with_title_bar,\n",
    "        \"Wrist camera view\",\n",
    "        (10, 40),  # Position of the text\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,  # Font type\n",
    "        1.5,  # Font scale (increased to match the \"Proprioceptive state\" title)\n",
    "        (255, 255, 255),  # Text color (white)\n",
    "        2,  # Text thickness\n",
    "        cv2.LINE_AA,  # Line type for better rendering\n",
    "    )\n",
    "    img_with_titles = cv2.putText(\n",
    "        img_with_titles,\n",
    "        \"Front camera view\",\n",
    "        (img1.shape[1] + 10, 40),  # Position of the text\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,  # Font type\n",
    "        1.5,  # Font scale (increased to match the \"Proprioceptive state\" title)\n",
    "        (255, 255, 255),  # Text color (white)\n",
    "        2,  # Text thickness\n",
    "        cv2.LINE_AA,  # Line type for better rendering\n",
    "    )\n",
    "\n",
    "    # Add black pixels at the bottom of the image\n",
    "    black_bar_height = 100\n",
    "    black_bar = np.zeros(\n",
    "        (black_bar_height, img_with_titles.shape[1], 3), dtype=np.uint8\n",
    "    )\n",
    "    img_with_bar = np.vstack((img_with_titles, black_bar))\n",
    "\n",
    "    # Add the \"Proprioceptive state\" text above the robot_state numbers\n",
    "    img_with_text = cv2.putText(\n",
    "        img_with_bar,\n",
    "        \"Proprioceptive state\",\n",
    "        (10, img_with_bar.shape[0] - 60),  # Position of the text\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,  # Font type\n",
    "        1.5,  # Font scale\n",
    "        (255, 255, 255),  # Text color (white)\n",
    "        2,  # Text thickness\n",
    "        cv2.LINE_AA,  # Line type for better rendering\n",
    "    )\n",
    "\n",
    "    # Format robot_state with consistent spacing\n",
    "    robot_state_text = (\n",
    "        \"[\" + \", \".join([\"{:6.2f}\".format(val) for val in robot_state]) + \"]\"\n",
    "    )\n",
    "    img_with_text = cv2.putText(\n",
    "        img_with_text,\n",
    "        robot_state_text,\n",
    "        (10, img_with_bar.shape[0] - 20),  # Position of the text\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,  # Font type\n",
    "        1,  # Font scale\n",
    "        (255, 255, 255),  # Text color (white)\n",
    "        2,  # Text thickness\n",
    "        cv2.LINE_AA,  # Line type for better rendering\n",
    "    )\n",
    "\n",
    "    # Add the \"2x\" text overlay in the lower-right corner\n",
    "    img_with_text = cv2.putText(\n",
    "        img_with_text,\n",
    "        f\"{fps//10}x\",\n",
    "        (\n",
    "            img_with_text.shape[1] - 100,\n",
    "            img_with_text.shape[0] - 40,\n",
    "        ),  # Adjusted position of the text\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,  # Font type\n",
    "        2,  # Font scale (doubled)\n",
    "        (255, 255, 255),  # Text color (white)\n",
    "        4,  # Text thickness (doubled)\n",
    "        cv2.LINE_AA,  # Line type for better rendering\n",
    "    )\n",
    "\n",
    "    imgs.append(img_with_text)\n",
    "\n",
    "imgs = np.array(imgs)\n",
    "create_mp4_jupyter(imgs, base_dir / f\"rollout_round_table_wrist.mp4\", fps=fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rerender old rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gym import get_env\n",
    "\n",
    "from pathlib import Path\n",
    "from src.visualization.render_mp4 import (\n",
    "    mp4_from_pickle_jupyter,\n",
    "    create_mp4_jupyter,\n",
    "    unpickle_data,\n",
    "    pickle_data,\n",
    ")\n",
    "from src.common.files import get_raw_paths\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from isaacgym import gymtorch, gymapi\n",
    "import torch\n",
    "\n",
    "\n",
    "base_dir = Path(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env(\n",
    "    gpu_id=0,\n",
    "    furniture=\"one_leg\",\n",
    "    num_envs=1,\n",
    "    randomness=\"low\",\n",
    "    resize_img=False,\n",
    "    april_tags=False,\n",
    "    verbose=False,\n",
    "    headless=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "# Perform random actions\n",
    "for _ in range(10):\n",
    "    action = torch.rand(8) * 2 - 1\n",
    "    env.step(action.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_raw_paths(\n",
    "    environment=\"sim\",\n",
    "    demo_source=\"teleop\",\n",
    "    demo_outcome=\"success\",\n",
    "    task=\"one_leg\",\n",
    "    randomness=\"low\",\n",
    ")\n",
    "\n",
    "path = sorted(paths, reverse=True)[0]\n",
    "\n",
    "print(f\"Rendering {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = unpickle_data(path)\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs = env.reset()\n",
    "imgs = []\n",
    "\n",
    "root_positions = env.root_tensor[:, 0:3]\n",
    "root_orientations = env.root_tensor[:, 3:7]\n",
    "root_linvels = env.root_tensor[:, 7:10]\n",
    "root_angvels = env.root_tensor[:, 10:13]\n",
    "\n",
    "dof_pos = env.dof_pos\n",
    "dof_vel = env.dof_vel\n",
    "\n",
    "rb_states = env.rb_states\n",
    "\n",
    "num_actors = env.root_tensor.shape[0]\n",
    "\n",
    "for obs in tqdm(data[\"observations\"]):\n",
    "    imgs.append(new_obs[\"color_image2\"].squeeze().cpu().numpy())\n",
    "\n",
    "    # Set the root state directly\n",
    "    offsets = (\n",
    "        torch.tensor([0, 0.0, 0.1]).repeat(num_actors).reshape(-1, 3).float().cuda()\n",
    "    )\n",
    "    root_positions += offsets\n",
    "    env.isaac_gym.set_actor_root_state_tensor(\n",
    "        env.sim, gymtorch.unwrap_tensor(env.root_tensor)\n",
    "    )\n",
    "\n",
    "    # Set the DOF state directly\n",
    "    # dof_pos += 0.1\n",
    "    # env.isaac_gym.set_dof_state_tensor(env.sim, gymtorch.unwrap_tensor(env.dof_states))\n",
    "\n",
    "    # Set the rigid body state directly\n",
    "    # rb_states[:, 0:3] += 0.1\n",
    "    # env.isaac_gym.set_rigid_body_state_tensor(env.sim, gymtorch.unwrap_tensor(rb_states))\n",
    "\n",
    "    # Update all the renderers\n",
    "    env.isaac_gym.fetch_results(env.sim, True)\n",
    "    env.isaac_gym.step_graphics(env.sim)\n",
    "\n",
    "    # # Refresh tensors\n",
    "    env.isaac_gym.refresh_dof_state_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_dof_force_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_rigid_body_state_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_jacobian_tensors(env.sim)\n",
    "    env.isaac_gym.refresh_mass_matrix_tensors(env.sim)\n",
    "\n",
    "    env.isaac_gym.render_all_camera_sensors(env.sim)\n",
    "    env.isaac_gym.start_access_image_tensors(env.sim)\n",
    "\n",
    "    env.isaac_gym.end_access_image_tensors(env.sim)\n",
    "\n",
    "    # Get the image\n",
    "    new_obs = env.get_observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the video\n",
    "imgs = np.array(imgs)\n",
    "create_mp4_jupyter(imgs, base_dir / f\"teleop_highres_one_leg_2.mp4\", fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.isaac_gym.get_sim_dof_count(env.sim), env.isaac_gym.get_sim_rigid_body_count(\n",
    "    env.sim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.isaac_gym.fetch_results(self.sim, True)\n",
    "# self.isaac_gym.step_graphics(self.sim)\n",
    "\n",
    "# # Refresh tensors.\n",
    "# self.isaac_gym.refresh_dof_state_tensor(self.sim)\n",
    "# self.isaac_gym.refresh_dof_force_tensor(self.sim)\n",
    "# self.isaac_gym.refresh_rigid_body_state_tensor(self.sim)\n",
    "# self.isaac_gym.refresh_jacobian_tensors(self.sim)\n",
    "# self.isaac_gym.refresh_mass_matrix_tensors(self.sim)\n",
    "# self.isaac_gym.render_all_camera_sensors(self.sim)\n",
    "\n",
    "# self.isaac_gym.start_access_image_tensors(self.sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_robot_state(observations):\n",
    "    robot_states = [obs[\"robot_state\"] for obs in observations]\n",
    "    parts_poses = [obs[\"parts_poses\"] for obs in observations]\n",
    "\n",
    "    # Remove the first robot_state entry\n",
    "    robot_states = robot_states[3:]\n",
    "\n",
    "    # Align the robot_states with parts_poses\n",
    "    aligned_observations = []\n",
    "    for i in range(len(robot_states)):\n",
    "        aligned_obs = {\n",
    "            \"robot_state\": robot_states[i],\n",
    "            \"color_image1\": observations[i][\"color_image1\"],\n",
    "            \"color_image2\": observations[i][\"color_image2\"],\n",
    "            \"parts_poses\": parts_poses[i],\n",
    "        }\n",
    "        aligned_observations.append(aligned_obs)\n",
    "\n",
    "    return aligned_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "imgs = []\n",
    "\n",
    "observations = data[\"observations\"]\n",
    "observations = offset_robot_state(observations)\n",
    "\n",
    "for obs in tqdm(data[\"observations\"]):\n",
    "    # Set the desired state\n",
    "    env.reset_env_to(env_idx=0, state=obs)\n",
    "\n",
    "    # torque_action = torch.zeros_like(env.dof_pos)\n",
    "    # env.isaac_gym.set_dof_actuation_force_tensor(\n",
    "    #     env.sim, gymtorch.unwrap_tensor(torque_action)\n",
    "    # )\n",
    "\n",
    "    # num_dofs = env.isaac_gym.get_sim_dof_count(env.sim)\n",
    "    # actions = torch.zeros(num_dofs).float().cuda()\n",
    "    # env.isaac_gym.set_dof_actuation_force_tensor(env.sim, gymtorch.unwrap_tensor(actions))\n",
    "\n",
    "    env.isaac_gym.simulate(env.sim)\n",
    "\n",
    "    # env.reset_env_to(env_idx=0, state=obs)\n",
    "\n",
    "    # Update all the renderers\n",
    "    env.isaac_gym.fetch_results(env.sim, True)\n",
    "    env.isaac_gym.step_graphics(env.sim)\n",
    "\n",
    "    # Refresh tensors\n",
    "    env.isaac_gym.refresh_dof_state_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_dof_force_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_rigid_body_state_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_jacobian_tensors(env.sim)\n",
    "    env.isaac_gym.refresh_mass_matrix_tensors(env.sim)\n",
    "\n",
    "    # env.isaac_gym.refresh_actor_root_state_tensor(env.sim)\n",
    "\n",
    "    env.isaac_gym.render_all_camera_sensors(env.sim)\n",
    "    env.isaac_gym.start_access_image_tensors(env.sim)\n",
    "\n",
    "    env.isaac_gym.end_access_image_tensors(env.sim)\n",
    "\n",
    "    # Get the image\n",
    "    obs = env.get_observation()\n",
    "\n",
    "    imgs.append(obs[\"color_image2\"].squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "imgs = []\n",
    "\n",
    "for obs in tqdm(data[\"observations\"]):\n",
    "    # env.reset_env_to(env_idx=0, state=obs)\n",
    "\n",
    "    # Set the desired state for robot joint positions\n",
    "    dof_pos = (\n",
    "        torch.from_numpy(\n",
    "            np.concatenate(\n",
    "                [\n",
    "                    obs[\"robot_state\"][\"joint_positions\"],\n",
    "                    np.array([obs[\"robot_state\"][\"gripper_width\"] / 2] * 2),\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "        .float()\n",
    "        .cuda()\n",
    "    )  # Convert the tensor to float32\n",
    "    # env.reset_franka_all(dof_pos)\n",
    "    dof_vel = torch.zeros_like(dof_pos).cuda()  # Set DOF velocities to zero\n",
    "\n",
    "    # Concatenate dof_pos and dof_vel along the second dimension\n",
    "    dof_state = torch.stack((dof_pos, dof_vel), dim=1)\n",
    "\n",
    "    # Get the actor index correctly\n",
    "    actor_idx = env.franka_actor_idxs_all_t[0].reshape(1, 1).cuda()\n",
    "\n",
    "    env.isaac_gym.set_dof_state_tensor_indexed(\n",
    "        env.sim, gymtorch.unwrap_tensor(dof_state), gymtorch.unwrap_tensor(actor_idx), 1\n",
    "    )\n",
    "\n",
    "    # Set the desired state for parts poses\n",
    "    parts_poses = (\n",
    "        torch.from_numpy(obs[\"parts_poses\"]).float().cuda()\n",
    "    )  # Convert the tensor to float32\n",
    "    env.isaac_gym.set_actor_root_state_tensor(\n",
    "        env.sim, gymtorch.unwrap_tensor(parts_poses)\n",
    "    )\n",
    "\n",
    "    # Update all the renderers\n",
    "    env.isaac_gym.fetch_results(env.sim, True)\n",
    "    env.isaac_gym.step_graphics(env.sim)\n",
    "\n",
    "    # Refresh tensors\n",
    "    env.isaac_gym.refresh_dof_state_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_dof_force_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_rigid_body_state_tensor(env.sim)\n",
    "    env.isaac_gym.refresh_jacobian_tensors(env.sim)\n",
    "    env.isaac_gym.refresh_mass_matrix_tensors(env.sim)\n",
    "\n",
    "    env.isaac_gym.render_all_camera_sensors(env.sim)\n",
    "    env.isaac_gym.start_access_image_tensors(env.sim)\n",
    "\n",
    "    env.isaac_gym.end_access_image_tensors(env.sim)\n",
    "\n",
    "    # Get the image\n",
    "    obs = env.get_observation()\n",
    "\n",
    "    imgs.append(obs[\"color_image2\"].squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main results bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Increase font size globally and set font to Times New Roman\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"font.size\": 12,  # Adjust the base font size as needed\n",
    "        \"font.family\": \"Times New Roman\",\n",
    "        \"legend.fontsize\": \"large\",  # Adjust legend font size\n",
    "        \"axes.labelsize\": \"large\",  # Adjust axis labels font size\n",
    "        \"axes.titlesize\": \"x-large\",  # Adjust axis title font size\n",
    "        \"xtick.labelsize\": \"large\",  # Adjust X-axis tick label size\n",
    "        \"ytick.labelsize\": \"large\",  # Adjust Y-axis tick label size\n",
    "    }\n",
    ")\n",
    "\n",
    "# Original data from the LaTeX table\n",
    "data = {\n",
    "    \"Method\": [\n",
    "        \"MLP-NC\",\n",
    "        \"MLP-C\",\n",
    "        \"DP-BC\",\n",
    "        \"State noise\",\n",
    "        \"Traj. Aug.\",\n",
    "        \"Col.-Inf.\",\n",
    "        \"TA & CI\",\n",
    "        \"Multi-task\",\n",
    "    ],\n",
    "    \"One leg Avg\": [0, 40, 59, 66, 66, 75, 76, 59],\n",
    "    \"One leg Max\": [0, 52, 68, 70, 73, 79, 83, 63],\n",
    "    \"Round table Avg\": [0, 9, 18, 9, 28, 24, 32, 25],\n",
    "    \"Round table Max\": [0, 15, 21, 10, 33, 27, 35, 35],\n",
    "    \"Lamp Avg\": [None, 3, 6, 6, 9, 18, 29, 14],\n",
    "    \"Lamp Max\": [None, 4, 7, 11, 12, 29, 35, 18],\n",
    "    \"Square table Avg\": [None, 2, 6, 10, 9, 12, 15, 7],\n",
    "    \"Square table Max\": [None, 2, 8, 11, 17, 19, 17, 10],\n",
    "}\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df = df.set_index(\"Method\")\n",
    "\n",
    "# Melt the DataFrame to have a suitable form for seaborn's barplot\n",
    "df_melted = df.reset_index().melt(\n",
    "    id_vars=\"Method\", var_name=\"Task\", value_name=\"Success Rate\"\n",
    ")\n",
    "\n",
    "# Add 'Type' column to differentiate between Avg and Max\n",
    "df_melted[\"Metric\"] = df_melted[\"Task\"].str.extract(r\"(\\bAvg|\\bMax)\")\n",
    "df_melted[\"Task\"] = df_melted[\"Task\"].str.replace(r\" Avg\", \"\").str.replace(r\" Max\", \"\")\n",
    "\n",
    "# Plot the grouped barplot using catplot\n",
    "g = sns.catplot(\n",
    "    x=\"Task\",\n",
    "    y=\"Success Rate\",\n",
    "    hue=\"Method\",\n",
    "    # row=\"Metric\",\n",
    "    col=\"Metric\",\n",
    "    sharey=False,\n",
    "    data=df_melted,\n",
    "    kind=\"bar\",\n",
    "    # height=4,\n",
    "    # aspect=2,\n",
    "    height=5.0,  # 5.0 good\n",
    "    aspect=1.3,  # 1.0 good-ish\n",
    "    palette=\"husl\",\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "# Set titles and labels\n",
    "g.fig.subplots_adjust(top=0.9)  # adjust the Figure in rp\n",
    "# g.fig.suptitle('Success Rates (%) of Methods Across Tasks') # - Average vs Maximum')\n",
    "# g.set_titles(\"{col_name} {col_var}\")\n",
    "\n",
    "# Iterate over each subplot in the FacetGrid to set y-axis labels\n",
    "for ax, metric in zip(g.axes.flat, [\"Avg\", \"Max\"]):\n",
    "    ax.set_ylim([0, 82])\n",
    "    ax.set_ylabel(f\"{metric} Success Rate (%)\")\n",
    "    # ax.set_xlabel(f'Task')\n",
    "    ax.set_xlabel(f\"\")\n",
    "    ax.set_title(f\"\")\n",
    "\n",
    "    # Get current labels\n",
    "    labels = [label.get_text() for label in ax.get_xticklabels()]\n",
    "    # Replace spaces with newline characters as needed\n",
    "    new_labels = [label.replace(\" \", \"\\n\") for label in labels]\n",
    "    # Set new labels\n",
    "    ax.set_xticklabels(\n",
    "        new_labels\n",
    "    )  # , rotation=45)  # Optional: adjust rotation for better readability\n",
    "\n",
    "\n",
    "# Create the legend manually (TODO)\n",
    "# Manually creating the legend\n",
    "# Get the unique methods from your DataFrame to create legend entries\n",
    "methods = df_melted[\"Method\"].unique()\n",
    "\n",
    "# Use seaborn's color palette for consistency with the plot\n",
    "palette = sns.color_palette(\"husl\", len(methods))\n",
    "\n",
    "# Create legend handles\n",
    "legend_handles = [\n",
    "    plt.Line2D([0], [0], color=palette[i], marker=\"o\", linestyle=\"\", label=method)\n",
    "    for i, method in enumerate(methods)\n",
    "]\n",
    "\n",
    "# Add the legend to the figure\n",
    "# g.fig.legend(handles=legend_handles, title='Method', bbox_to_anchor=(0.75, 0.95), loc='upper left')\n",
    "# g.fig.legend(handles=legend_handles, title='Method', bbox_to_anchor=(0.875, 1.05), loc='upper left')\n",
    "g.figure.legend(\n",
    "    handles=legend_handles,\n",
    "    title=\"Method\",\n",
    "    bbox_to_anchor=(0.85, 0.975),\n",
    "    loc=\"upper left\",\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Since we're adding a legend outside the plot, adjust the subplots to fit the figure area\n",
    "# g.fig.subplots_adjust(right=0.7)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(\"success_rate_barplot.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
