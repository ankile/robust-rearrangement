{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Loss: 16.067676544189453\n",
      "Vectorized Loss: 16.067676544189453\n",
      "The original approach and the vectorized approach produce the same result.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def original_approach(sim_emb1, real_emb1, sim_emb2, real_emb2):\n",
    "    real_emb1_expanded = real_emb1.unsqueeze(1)\n",
    "    sim_emb1_expanded = sim_emb1.unsqueeze(0)\n",
    "    differences1 = torch.norm((real_emb1_expanded - sim_emb1_expanded), dim=-1)\n",
    "\n",
    "    real_emb2_expanded = real_emb2.unsqueeze(1)\n",
    "    sim_emb2_expanded = sim_emb2.unsqueeze(0)\n",
    "    differences2 = torch.norm((real_emb2_expanded - sim_emb2_expanded), dim=-1)\n",
    "\n",
    "    loss = differences1.mean(dim=(0, 1)) + differences2.mean(dim=(0, 1))\n",
    "    return loss / 2\n",
    "\n",
    "def vectorized_approach(sim_emb1, real_emb1, sim_emb2, real_emb2):\n",
    "    sim_emb = torch.stack((sim_emb1, sim_emb2), dim=1)\n",
    "    real_emb = torch.stack((real_emb1, real_emb2), dim=1)\n",
    "\n",
    "    sim_emb_expanded = sim_emb.unsqueeze(1)\n",
    "    real_emb_expanded = real_emb.unsqueeze(0)\n",
    "\n",
    "    differences = torch.norm((real_emb_expanded - sim_emb_expanded), dim=-1)\n",
    "    loss = differences.mean(dim=(0, 1, 2))\n",
    "    return loss\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate random vectors\n",
    "N1, N2, D = 128, 128, 128\n",
    "sim_emb1 = torch.randn(N1, D)\n",
    "real_emb1 = torch.randn(N2, D)\n",
    "sim_emb2 = torch.randn(N1, D)\n",
    "real_emb2 = torch.randn(N2, D)\n",
    "\n",
    "# Compute the loss using the original approach\n",
    "original_loss = original_approach(sim_emb1, real_emb1, sim_emb2, real_emb2)\n",
    "\n",
    "# Compute the loss using the vectorized approach\n",
    "vectorized_loss = vectorized_approach(sim_emb1, real_emb1, sim_emb2, real_emb2)\n",
    "\n",
    "# Compare the results\n",
    "print(\"Original Loss:\", original_loss.item())\n",
    "print(\"Vectorized Loss:\", vectorized_loss.item())\n",
    "\n",
    "if torch.allclose(original_loss, vectorized_loss):\n",
    "    print(\"The original approach and the vectorized approach produce the same result.\")\n",
    "else:\n",
    "    print(\"The original approach and the vectorized approach produce different results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365 µs ± 5.84 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "original_approach(sim_emb1, real_emb1, sim_emb2, real_emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413 µs ± 23.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "vectorized_approach(sim_emb1, real_emb1, sim_emb2, real_emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed Latents from sim and real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import get_encoder\n",
    "from src.common.files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/scratch/ankile/miniconda3/envs/rr/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/data/scratch/ankile/miniconda3/envs/rr/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/scratch/ankile/miniconda3/envs/rr/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "r3m = get_encoder(\n",
    "    \"r3m_18\",\n",
    "    device=\"cuda\",\n",
    "    freeze=True,\n",
    "    pretrained=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
