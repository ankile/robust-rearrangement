{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import furniture_bench  # noqa: F401\n",
    "\n",
    "from src.behavior.base import Actor\n",
    "from src.eval.load_model import load_bc_actor\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from furniture_bench.envs.observation import DEFAULT_STATE_OBS\n",
    "import hydra\n",
    "from src.gym import turn_off_april_tags\n",
    "from src.gym.env_rl_wrapper import ResidualPolicyEnvWrapper\n",
    "from src.gym.furniture_sim_env import FurnitureRLSimEnv\n",
    "from src.models.residual import ResidualPolicy\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "turn_off_april_tags()\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_run = api.run(\"residual-ppo-2/runs/sdoolikr\")\n",
    "# residual_run = api.run(\"residual-ppo-2/runs/3iom50to\")\n",
    "\n",
    "cfg = OmegaConf.create(\n",
    "    {\n",
    "        **residual_run.config,\n",
    "        \"env\": {\"randomness\": \"low\"},\n",
    "        \"base_bc_poliy\": \"ankile/one_leg-diffusion-state-1/runs/7623y5vn\",\n",
    "    }\n",
    ")\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env: FurnitureRLSimEnv = FurnitureRLSimEnv(\n",
    "    act_rot_repr=cfg.act_rot_repr,\n",
    "    action_type=cfg.action_type,\n",
    "    april_tags=False,\n",
    "    concat_robot_state=True,\n",
    "    ctrl_mode=\"diffik\",\n",
    "    obs_keys=DEFAULT_STATE_OBS,\n",
    "    furniture=\"one_leg\",\n",
    "    gpu_id=0,\n",
    "    headless=True,  # cfg.headless,\n",
    "    num_envs=128,  # cfg.num_envs,\n",
    "    observation_space=\"state\",\n",
    "    randomness=cfg.env.randomness,\n",
    "    max_env_steps=100_000_000,\n",
    ")\n",
    "\n",
    "env.max_force_magnitude = 0.05\n",
    "env.max_torque_magnitude = 0.0025\n",
    "\n",
    "# Load the behavior cloning actor\n",
    "bc_actor: Actor = load_bc_actor(cfg.base_bc_poliy)\n",
    "\n",
    "env: ResidualPolicyEnvWrapper = ResidualPolicyEnvWrapper(\n",
    "    env,\n",
    "    max_env_steps=cfg.num_env_steps,\n",
    "    reset_on_success=cfg.reset_on_success,\n",
    "    reset_on_failure=cfg.reset_on_failure,\n",
    ")\n",
    "env.set_normalizer(bc_actor.normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Residual policy setup\n",
    "residual_policy: ResidualPolicy = hydra.utils.instantiate(\n",
    "    cfg.residual_policy,\n",
    "    obs_shape=env.observation_space.shape,\n",
    "    action_shape=env.action_space.shape,\n",
    ")\n",
    "\n",
    "# Load the residual policy weights\n",
    "wts = [f for f in residual_run.files() if \".pt\" in f.name][0]\n",
    "wts.download(replace=True)\n",
    "\n",
    "residual_policy.load_state_dict(torch.load(wts.name)[\"model_state_dict\"])\n",
    "\n",
    "residual_policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_obs = env.reset()\n",
    "bc_actor.reset()\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "\n",
    "for step in trange(0, 850):\n",
    "\n",
    "    # Get the base normalized action\n",
    "    base_naction = bc_actor.action_normalized(next_obs)\n",
    "\n",
    "    # Process the obs for the residual policy\n",
    "    next_obs = env.process_obs(next_obs)\n",
    "    next_residual_obs = torch.cat([next_obs, base_naction], dim=-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        residual_naction_samp, logprob, _, value, naction_mean = (\n",
    "            residual_policy.get_action_and_value(next_residual_obs)\n",
    "        )\n",
    "\n",
    "    residual_naction = naction_mean\n",
    "    naction = base_naction + residual_naction * cfg.residual_policy.action_scale\n",
    "\n",
    "    next_obs, reward, next_done, truncated, infos = env.step(naction)\n",
    "\n",
    "    total_reward += reward.sum()\n",
    "\n",
    "\n",
    "# Calculate the success rate\n",
    "total_reward / env.num_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only base policy: 51%\n",
    "# With ok residual: 87%\n",
    "# With better residual: 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
