{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Importing module 'gym_38' (/data/scratch/ankile/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)\n",
                        "Setting GYM_USD_PLUG_INFO_PATH to /data/scratch/ankile/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json\n",
                        "PyTorch version 2.3.0+cu121\n",
                        "Device count 1\n",
                        "/data/scratch/ankile/isaacgym/python/isaacgym/_bindings/src/gymtorch\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Using /data/scratch/ankile/.cache as PyTorch extensions root...\n",
                        "Emitting ninja build file /data/scratch/ankile/.cache/gymtorch/build.ninja...\n",
                        "Building extension module gymtorch...\n",
                        "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ninja: no work to do.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading extension module gymtorch...\n"
                    ]
                }
            ],
            "source": [
                "import furniture_bench  # noqa: F401\n",
                "\n",
                "from src.behavior.base import Actor\n",
                "from src.eval.load_model import load_bc_actor\n",
                "\n",
                "import wandb\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "from omegaconf import OmegaConf\n",
                "\n",
                "from furniture_bench.envs.observation import DEFAULT_STATE_OBS\n",
                "import hydra\n",
                "from src.gym import turn_off_april_tags\n",
                "from src.gym.env_rl_wrapper import ResidualPolicyEnvWrapper\n",
                "from furniture_bench.envs.furniture_rl_sim_env import FurnitureRLSimEnv\n",
                "from src.models.residual import ResidualPolicy\n",
                "\n",
                "from tqdm import trange\n",
                "\n",
                "\n",
                "turn_off_april_tags()\n",
                "\n",
                "api = wandb.Api()\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'env': {'randomness': 'low'}, 'seed': 2509922154, 'debug': False, 'gamma': 0.999, 'wandb': {'mode': 'online', 'entity': 'ankile', 'project': 'residual-ppo-dr-1'}, 'vf_coef': 0.5, 'ent_coef': 0, 'headless': True, 'norm_adv': True, 'num_envs': 1024, 'clip_coef': 0.2, 'target_kl': 0.1, 'batch_size': 716800, 'clip_vloss': True, 'eval_first': True, 'gae_lambda': 0.95, 'action_type': 'pos', 'clip_reward': 5, 'residual_l1': 0, 'residual_l2': 0.01, 'act_rot_repr': 'rot_6d', 'lr_scheduler': {'name': 'cosine', 'warmup_steps': 8}, 'base_bc_poliy': 'ol-state-dr-1/r9wm1uo6', 'eval_interval': 5, 'max_grad_norm': 0.5, 'num_env_steps': 700, 'update_epochs': 20, 'minibatch_size': 716800, 'num_iterations': 418, 'num_minibatches': 1, 'residual_policy': {'_target_': 'src.models.residual.ResidualPolicy', 'init_logstd': -1.5, 'action_scale': 0.1, 'action_head_std': 0, 'actor_activation': 'ReLU', 'actor_num_layers': 2, 'actor_hidden_size': 256, 'critic_activation': 'ReLU', 'critic_num_layers': 2, 'critic_hidden_size': 256, 'critic_last_layer_std': 0.25, 'critic_last_layer_bias_const': 0.25}, 'total_timesteps': 300000000, 'normalize_reward': False, 'reset_on_failure': False, 'reset_on_success': False, 'truncation_as_done': True, 'checkpoint_interval': 10, 'learning_rate_actor': 0.0005, 'torch_deterministic': True, 'learning_rate_critic': 0.005, 'data_collection_steps': 700, 'reset_every_iteration': True, 'n_iterations_train_only_value': 0}"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "residual_run = api.run(\"residual-ppo-dr-1/cclmq6hq\")\n",
                "# residual_run = api.run(\"residual-ppo-2/runs/3iom50to\")\n",
                "\n",
                "cfg = OmegaConf.create(\n",
                "    {\n",
                "        **residual_run.config,\n",
                "        \"env\": {\"randomness\": \"low\"},\n",
                "        # \"base_bc_poliy\": \"ol-state-dr-1/r9wm1uo6\",\n",
                "    }\n",
                ")\n",
                "\n",
                "cfg"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Making DiffIK controller with pos_scalar: 1.0, rot_scalar: 1.0\n",
                        "Stiffness: 1000.0, Damping: 200.0\n",
                        "Observation keys: ['robot_state/ee_pos', 'robot_state/ee_quat', 'robot_state/ee_pos_vel', 'robot_state/ee_ori_vel', 'robot_state/gripper_width', 'parts_poses']\n",
                        "Not connected to PVD\n",
                        "+++ Using GPU PhysX\n",
                        "Physics Engine: PhysX\n",
                        "Physics Device: cuda:0\n",
                        "GPU Pipeline: enabled\n",
                        "Using SDF cache directory '/afs/csail.mit.edu/u/a/ankile/.isaacgym/sdf_V100'\n",
                        "~!~!~! Loaded/Cooked SDF triangle mesh 0 @ 0xcbb27940, resolution=512, spacing=0.000317\n",
                        "  ~!~! Bounds:  (-0.081250, 0.081250) (-0.015685, 0.015565) (-0.081250, 0.081251)\n",
                        "  ~!~! Extents: (0.162500, 0.031250, 0.162501)\n",
                        "  ~!~! Resolution: (512, 99, 512)\n",
                        "~!~!~! Loaded/Cooked SDF triangle mesh 1 @ 0xcc3e56d0, resolution=512, spacing=0.000171\n",
                        "  ~!~! Bounds:  (-0.015000, 0.015000) (-0.056250, 0.031250) (-0.014383, 0.015618)\n",
                        "  ~!~! Extents: (0.030000, 0.087500, 0.030001)\n",
                        "  ~!~! Resolution: (176, 512, 176)\n",
                        "~!~!~! Loaded/Cooked SDF triangle mesh 2 @ 0xcd755050, resolution=512, spacing=0.000172\n",
                        "  ~!~! Bounds:  (-0.015000, 0.015000) (-0.056562, 0.031376) (-0.015438, 0.014563)\n",
                        "  ~!~! Extents: (0.030000, 0.087938, 0.030001)\n",
                        "  ~!~! Resolution: (175, 512, 175)\n",
                        "~!~!~! Loaded/Cooked SDF triangle mesh 3 @ 0xcec1ce00, resolution=512, spacing=0.000171\n",
                        "  ~!~! Bounds:  (-0.015000, 0.015000) (-0.056250, 0.031250) (-0.014375, 0.015625)\n",
                        "  ~!~! Extents: (0.030000, 0.087500, 0.030000)\n",
                        "  ~!~! Resolution: (176, 512, 176)\n",
                        "~!~!~! Loaded/Cooked SDF triangle mesh 4 @ 0xce3dfa80, resolution=512, spacing=0.000171\n",
                        "  ~!~! Bounds:  (-0.015000, 0.015000) (-0.056250, 0.031250) (-0.015618, 0.014383)\n",
                        "  ~!~! Extents: (0.030000, 0.087500, 0.030001)\n",
                        "  ~!~! Resolution: (176, 512, 176)\n",
                        "Making DiffIK controller with pos_scalar: 1.0, rot_scalar: 1.0\n",
                        "Sim steps: 6\n",
                        "Max force magnitude: 0.2 Max torque magnitude: 0.005 Obstacle range: 0.02 Franka joint randomization limit: 0.08726646259971647\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/data/scratch/ankile/miniconda3/envs/rr/lib/python3.8/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
                        "  torch.utils._pytree._register_pytree_node(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total parameters: 8.98M\n",
                        "normalizer: 0.00M parameters\n",
                        "loss_fn: 0.00M parameters\n",
                        "model: 8.98M parameters\n",
                        "./models/legendary-sunset-7/actor_chkpt_best_success_rate.pt\n"
                    ]
                }
            ],
            "source": [
                "env: FurnitureRLSimEnv = FurnitureRLSimEnv(\n",
                "    act_rot_repr=cfg.act_rot_repr,\n",
                "    action_type=cfg.action_type,\n",
                "    april_tags=False,\n",
                "    concat_robot_state=True,\n",
                "    ctrl_mode=\"diffik\",\n",
                "    obs_keys=DEFAULT_STATE_OBS,\n",
                "    furniture=\"one_leg\",\n",
                "    gpu_id=0,\n",
                "    headless=True,  # cfg.headless,\n",
                "    num_envs=1024,  # cfg.num_envs,\n",
                "    observation_space=\"state\",\n",
                "    randomness=cfg.env.randomness,\n",
                "    max_env_steps=100_000_000,\n",
                ")\n",
                "\n",
                "env.max_force_magnitude = 0.05\n",
                "env.max_torque_magnitude = 0.0025\n",
                "\n",
                "# Load the behavior cloning actor\n",
                "bc_actor: Actor = load_bc_actor(cfg.base_bc_poliy)\n",
                "\n",
                "env: ResidualPolicyEnvWrapper = ResidualPolicyEnvWrapper(\n",
                "    env,\n",
                "    max_env_steps=cfg.num_env_steps,\n",
                "    reset_on_success=cfg.reset_on_success,\n",
                "    reset_on_failure=cfg.reset_on_failure,\n",
                ")\n",
                "env.set_normalizer(bc_actor.normalizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total parameters: 0.17M\n",
                        "actor_mean: 0.09M parameters\n",
                        "critic: 0.08M parameters\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "ResidualPolicy(\n",
                            "  (actor_mean): Sequential(\n",
                            "    (0): Linear(in_features=68, out_features=256, bias=True)\n",
                            "    (1): ReLU()\n",
                            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
                            "    (3): ReLU()\n",
                            "    (4): Linear(in_features=256, out_features=10, bias=False)\n",
                            "  )\n",
                            "  (critic): Sequential(\n",
                            "    (0): Linear(in_features=68, out_features=256, bias=True)\n",
                            "    (1): ReLU()\n",
                            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
                            "    (3): ReLU()\n",
                            "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
                            "  )\n",
                            ")"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "\n",
                "# Residual policy setup\n",
                "residual_policy: ResidualPolicy = hydra.utils.instantiate(\n",
                "    cfg.residual_policy,\n",
                "    obs_shape=env.observation_space.shape,\n",
                "    action_shape=env.action_space.shape,\n",
                ")\n",
                "\n",
                "# Load the residual policy weights\n",
                "wts = [f for f in residual_run.files() if \".pt\" in f.name][0]\n",
                "wts.download(replace=True)\n",
                "\n",
                "residual_policy.load_state_dict(torch.load(wts.name)[\"model_state_dict\"])\n",
                "\n",
                "residual_policy.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 100/100 [01:41<00:00,  1.01s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CPU times: user 2min 12s, sys: 1min 31s, total: 3min 43s\n",
                        "Wall time: 1min 43s\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "tensor(0., device='cuda:0')"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "\n",
                "next_obs = env.reset()\n",
                "bc_actor.reset()\n",
                "\n",
                "total_reward = 0\n",
                "\n",
                "for step in trange(0, 100):\n",
                "\n",
                "    # Get the base normalized action\n",
                "    base_naction = bc_actor.action_normalized(next_obs)\n",
                "\n",
                "    # Process the obs for the residual policy\n",
                "    next_obs = env.process_obs(next_obs)\n",
                "    next_residual_obs = torch.cat([next_obs, base_naction], dim=-1)\n",
                "\n",
                "    with torch.no_grad():\n",
                "        residual_naction_samp, logprob, _, value, naction_mean = (\n",
                "            residual_policy.get_action_and_value(next_residual_obs)\n",
                "        )\n",
                "\n",
                "    residual_naction = naction_mean\n",
                "    naction = base_naction + residual_naction * cfg.residual_policy.action_scale\n",
                "\n",
                "    next_obs, reward, next_done, truncated, infos = env.step(naction)\n",
                "\n",
                "    total_reward += reward.sum()\n",
                "\n",
                "\n",
                "# Calculate the success rate\n",
                "total_reward / env.num_envs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Only base policy: 51%\n",
                "# With ok residual: 87%\n",
                "# With better residual: 95%"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Wall time: 37.8 s\n",
                "# Wall time: 1min 43s"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "rr",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}